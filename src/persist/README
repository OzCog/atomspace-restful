                          Persist
                          -------
                Linas Vepstas, Feb-April 2008

A simple-minded implementation of atom persistence into SQL.

STATUS:
Functional but incomplete prototype. Save and restore of atoms, 
and simple truth values, works. Attempts to stay in sync with
handles as stored in the TLB. Can bulk-save and bulk-restore
the entire contents of an AtomTable.

ToDo: add incremental save/restore.
Add full support for other truth values.
Add full support for attention values. (??)
Add full support for Atom deletion.

Goals:
------
The goal of this implementation is to:

1) Simplify Linas' day-to-day use of opencog. The problem is that
   loading data, by parsing english sentences, into opencog is slow.
   Every time opencog source is changed, the data needs to be reloaded.
   This makes development tedious.  The hope is that data persistence
   will aleviate data load time.

2) Make sure that the opencog core desisgn is amenable to incremental,
   just-in-time data persistence; that is, the fetching of data 
   as it is needed, and saving it away when its not needed. This
   requires an infrastrcture for attention allocation (atoms
   that don't get attention can be saved away to disk, while those
   that are needed are fetched on demand.) This will also require 
   infrastructure for locks, use counts, etc. that are typical
   of multi-htreded programming.  This prototype should lay the 
   foundations for more sophisticated schemes (non-SQL-based) 
   to take its place.

3) Provide a baseline/reference implementation by which other 
   persistence designs can be measured.

Design:
-------
The core design will use a few very simple SQL tables, and some
simple readers and writers to save and restore atoms from an SQL 
database.

Note that the core design does *not* make use of object reflection, 
nor can it store arbitrary kinds of objects. It is very definitely 
hard-wired. Yes, this can be considered to be a short-coming. 
A more general, persistent object framework (for C) can be found 
at http://estron.alioth.debian.org/  However, simplicity, at the
cost of missing flexibility, seems more important.

The current design can save/restore individual atoms, and it can
bulk-save/bulk-restore the entire contents of an AtomTable.
As such, if offers little or no advantage over flat-file storage.
A so-far unrealized goal of the prototype is to implement 
incremental save and restore -- that is, to fetch atoms in a 
"just in time" fashion, and to save away atoms that are not 
needed in RAM (e.g. atoms with low/non-existent attention values).

However, the current AtomTable implementation makes it hard for
the incremental approach to be pursued.  For example, the incoming
set of an atom is expected to be fully populated; and the current
manner in which this is done requires atoms to be fully instantiated.
This makes it somewhat hard to have only a part of a graph in RAM.
However, this is solvable with a bit of elbow grease.

Caveats:
--------
Its not at all clear whether this is a good idea; there are several
major drawbacks to using SQL. These include:
-- Large performance overhead, even if an embedded SQL server were to be used.
-- If/when incremental save/restore is implemented, then there's potentially 
   large churn, if short-lived atoms are constantly created/destroyed.

Getting started
---------------
Create a database called "OpenCog"; for example, in postgres, at
the unix command line:

   $ createdb opencog

(you may need to su - postgres; createuser <username> first)
(You may need to edit pg_hba.conf)

Then create the database tables:

   $ cat atom.sql | psql opencog

Next, set up the correct ODBC connection. This version uses iODBC
So edit .odbc.ini in your home directory, and add a stanza similar
to this, adjusting for user and passwd appropriately.  The password
to be supplied is the database password, not the user password.
The database password can be set by doing:

   $ psql -c "ALTER USER linas WITH PASSWORD 'asdf'" -d template1

[opencog]
Description    = PostgreSQL
Driver      = PostgreSQL
Trace    = No
TraceFile      =
Database    = opencog
Servername     = localhost
Port     = 5432
Username    = linas
Password    = asdf
ReadOnly    = No
RowVersioning     = No
ShowSystemTables     = Yes
ShowOidColumn     = Yes
FakeOidIndex      = Yes
ConnSettings      =


Experimental Diary & Results
----------------------------
First run with a large data set (1564K atoms) was a disaster.
Huge CPU usage, with 75% of cpu usage occuring in the kernel block i/o
layer, and 12% each for the opencog and postgres times:
   112:00 [md4_raid1] or 4.3 millisecs per atom
   17 minutes for postgres, and opencog, each. or 0.66 milisecs per atom
   1937576 - 1088032 kB = 850MBytes disk use

Experiment: is this due to the bad design for trying to figure whether
"INSERT" or "UPDATE" should be used. A local client-side cache of the 
keys in the DB seems to change little:

   cpu usage for postgres 11:04  and opencog 10:40 and 112:30 for md

So this change drops postgres server and opencog cpu usage
significantly, but the insance kernel CPU usage remains.

The above disaster can be attributed to bad defaults for the postgres
server. In particular, sync to disk, while critical for commercial
database use, is pointless for current use. Also, buffer sizes are much
too small. Edit postgresql.conf and make the following changes:

   shared_buffers = default was 24MB, change to 384MB
   work_mem = default was 1MB change to 32MB
   fsync = default on  change to off
   synchronous_commit = default on change to off
   wal_buffers = default 64kB change to 512kB
   commit_delay = default 0 change to 10000 (10K) microseconds
   ssl = default true change to false

Restarting the server might lead to errors stating that max shared mem
usage has been exceeded. This can be fixed by:

   vi /etc/sysctl.conf
   kernel.shmmax = 440100100
(save file contents, then)
   sysctl -p /etc/sysctl.conf

Result:
   cogserver = 10:45 mins = 0.41  millisecs/atom (2.42K atoms/sec)
   postgres  =  7:32 mins = 0.29  millisecs/atom (2.65K atoms/sec)
   md        =  0:42 mins = 0.026 millisecs/atom (37K atoms/sec)


try again with begn; commit;  
use analyze;

use prepare

   2:34 start

Loading performance. Database contains 1564K Atoms, and 2413K edges.
CPU usage:
2:08 postgres = 82 microsecs/atom (12.2K atoms/sec)
similar to for opencog, but then atomtable needs to reconcile, which
takes an additional 8:30 minutes to attach incoming handles!!

Conclude: database loading would be much faster if we loaded all of
the atoms first, then all of the lowest-level links, etc.  This assumes
tht opencog is strictly hierarchically structured.



TODO
----
-- Store more complex truth value types

-- Store attention values.

-- Loading from SQL uses half as much ram as loading from XML, which
   suggests the XML loader has a memory leak!?  Need to run valgrind on
   it.

-- Investigate "memcachedb" as an alternative?
